{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6995432",
   "metadata": {},
   "source": [
    "### Setup packages\n",
    "First we will suppress all the info messages for clearer output and import right packages; TensorFlow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05bb9c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1760992772.225215 2874020 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1760992772.228146 2874020 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1760992772.234752 2874020 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760992772.234760 2874020 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760992772.234761 2874020 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1760992772.234762 2874020 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "/vol/home/s2631415/.conda/envs/myenv/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #Suppress info messages for clearer output\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "# Import NNs and some aditional keras functions for the testing of different models\n",
    "from typing import cast, Any\n",
    "from keras.layers import Flatten, Dense, Conv2D, MaxPool2D, AvgPool2D, Dropout\n",
    "from keras.optimizers import SGD, AdamW\n",
    "from keras.initializers import HeNormal, GlorotNormal, RandomNormal, RandomUniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a0b2a",
   "metadata": {},
   "source": [
    "### Load in the MNIST-fashion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "408eb41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "#Create training set and validation set\n",
    "X_valid, X_train = X_train_full[:6000]/255.0, X_train_full[6000:]/255.0 #Confert 8-bit int to floating point in [0,1]\n",
    "y_valid, y_train = y_train_full[:6000], y_train_full[6000:] \n",
    "\n",
    "#Class names corresponding to label, which is represented with integer\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d31632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "model.add(keras.layers.Dense(300, 'relu'))\n",
    "model.add(keras.layers.Dense(100, 'relu'))\n",
    "model.add(keras.layers.Dense(10, 'softmax'))\n",
    "\n",
    "model.compile(optimizer=\"sgd\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df31890",
   "metadata": {},
   "source": [
    "### Optimize Hyperparameters (HP)\n",
    "\n",
    "Previous cells were used to learn how to setup a basic vanilla NN and a CNN. Our next task is to test different HP. Things that will be tested are initialization techniques, activation functions, optimizers and regularization. The goal is not to find the best accuracy but more learn what the effect is of some hyperparameters on these networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cf6b73",
   "metadata": {},
   "source": [
    "### Vanilla NN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "842357a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class vanillaNN():\n",
    "    \"\"\"Vanilla Neural Network class using Keras and Tensorflow for simpler implementation of HPO\"\"\"\n",
    "\n",
    "    def __init__(self, initialization=GlorotNormal(), activation='relu', optimizer:str|object='adamW', \n",
    "                 regularization='l1', dropout=0.5):\n",
    "        self.initialization = initialization\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        self.regularization = regularization\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.model = keras.Sequential([\n",
    "            keras.Input(shape=(28, 28)),\n",
    "            Flatten(),\n",
    "            self._make_layer(300),\n",
    "            Dropout(self.dropout),\n",
    "            self._make_layer(100),\n",
    "            Dropout(self.dropout),\n",
    "            Dense(10, 'softmax')\n",
    "        ])\n",
    "\n",
    "        self.model.compile(optimizer=cast(Any, self.optimizer), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def _make_layer(self, num_neurons):\n",
    "        # Cast initializer to Any to satisfy static type checkers\n",
    "        layer = Dense(num_neurons, activation=self.activation, kernel_initializer=cast(Any, self.initialization))\n",
    "        return layer\n",
    "\n",
    "    def fit_model(self, X, y, epochs=30, validation_data=None):\n",
    "        history = self.model.fit(X, y, epochs=epochs, validation_data=validation_data, verbose=1)\n",
    "        return history\n",
    "    \n",
    "    def _summary(self):\n",
    "        print(self.model.summary())\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fd67a3",
   "metadata": {},
   "source": [
    "### CNN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4eeadcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN():\n",
    "    \"\"\"Convolutional Neural Network class using Keras and Tensorflow for optimizing Hyperparameters\"\"\"\n",
    "    \n",
    "    def __init__(self, initialization=GlorotNormal(), activation='relu', optimizer:str|object='adamW', \n",
    "                 regularization='l1', dropout=0.5, input_shape=(28, 28, 1)):\n",
    "        self.initialization = initialization\n",
    "        self.activation = activation\n",
    "        self.optimizer = optimizer\n",
    "        self.regularization = regularization\n",
    "        self.dropout = dropout\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "        self.model = keras.models.Sequential([\n",
    "            keras.Input(shape=input_shape),\n",
    "            Conv2D(64, 5, activation=self.activation, padding='same'),\n",
    "            MaxPool2D(2),\n",
    "            self._make_layer(128, 3),\n",
    "            self._make_layer(128, 3),\n",
    "            MaxPool2D(2), \n",
    "            self._make_layer(256, 3),\n",
    "            self._make_layer(256, 3),\n",
    "            AvgPool2D(2),\n",
    "            Flatten(),\n",
    "            Dense(128, activation=self.activation),\n",
    "            Dropout(self.dropout),\n",
    "            Dense(64, activation=self.activation),\n",
    "            Dropout(self.dropout),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        self.model.compile(optimizer=cast(Any, self.optimizer), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def _make_layer(self, num_neurons, kernel_size):\n",
    "        layer = Conv2D(num_neurons, kernel_size=kernel_size, activation=self.activation, kernel_initializer=cast(Any, self.initialization), padding='same')\n",
    "        return layer\n",
    "\n",
    "    def fit_model(self, X, y, epochs=30, validation_data=None):\n",
    "        return self.model.fit(X, y, epochs=epochs, validation_data=validation_data, verbose=1)\n",
    "    \n",
    "    def _summary(self):\n",
    "        print(self.model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a2d3c",
   "metadata": {},
   "source": [
    "### Run vanilla NN for different HP configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b4a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_funcs = ['relu', 'tanh', 'sigmoid'] \n",
    "initializers = {HeNormal(): 'HeNormal', RandomNormal(): 'RandomNormal', GlorotNormal(): 'GlorotNormal'}\n",
    "regularizations = [None, 'l1', 'l2']\n",
    "dropouts = [0, 0.3, 0.5]\n",
    "learning_rates = [0.005, 0.01, 0.015, 0.02]\n",
    "\n",
    "len_loops = len(activation_funcs) * len(initializers) * len(regularizations) * len(dropouts) * len(learning_rates)\n",
    "i = 0\n",
    "final = pd.DataFrame([], columns=['optimizer', 'initializer', 'regularization', 'activation_function', 'best_valid_acc', 'last_valid_acc', 'last_train_acc', 'model'])\n",
    "for lr in learning_rates:\n",
    "    for initial in initializers.keys():\n",
    "        for regular in regularizations:\n",
    "            for active in activation_funcs:\n",
    "                for drops in dropouts:\n",
    "                        if (i % 10) == 0:\n",
    "                            print(f\"{i}/{len_loops}\")\n",
    "                        i+=1\n",
    "                        optimiz = AdamW(learning_rate=lr/10)\n",
    "                        model = vanillaNN(initialization=initial, activation=active, optimizer=optimiz, regularization=regular, dropout=drops)\n",
    "                        hist = model.fit_model(X_train, y_train, epochs=35, validation_data=(X_valid, y_valid))\n",
    "                        max_val = max(hist.history['val_accuracy'])\n",
    "                        last_val = hist.history['val_accuracy'][-1]\n",
    "                        last_train = hist.history['accuracy'][-1]\n",
    "\n",
    "                        setup_dict = pd.DataFrame([{'optimizer': 'AdamW', \n",
    "                            'initializer': initializers[initial],\n",
    "                            'regularization': regular,\n",
    "                            'activation_function': active,\n",
    "                            'dropout': str(drops),\n",
    "                            'lr': str(lr),\n",
    "                            'best_valid_acc': max_val,\n",
    "                            'last_valid_acc': last_val,\n",
    "                            'last_train_acc': last_train,\n",
    "                            'model': 'vanilla_NN'}])\n",
    "                \n",
    "                        final = pd.concat((final, setup_dict), ignore_index=True)\n",
    "\n",
    "final.to_csv('Configuration_performance_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c6627",
   "metadata": {},
   "source": [
    "### Statistical Test to see which parameters and first order combinations of pairs are the most importartent\n",
    "\n",
    "In our previous training of the model excecuted a full factorial design. Now we would like to test which variables have the greatest effect on the performance. For the main effects we will take the $ S(x_{i}) = \\frac{var(x_{i})}{var{\\bold{X}}}$, with $x_{i} \\subset \\bold{X} $. In words this describes is describes how much of the total variance on variable explains of the total variance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42cccfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla NN:  {'initializer': 0.0002443159386344149, 'regularization': 9.840915296498053e-05, 'activation_function': 0.42564803440853044, 'dropout': 0.2025321178295453, 'lr': 0.6688080810501781}\n",
      "CNN:  {'initializer': 0.0030408075982708256, 'regularization': 0.00010831147732210297, 'activation_function': 0.23724047201920068, 'dropout': 0.013452382087283693, 'optimizer': 0.3400032902768165}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Configuration_performance_file.csv', index_col=0)\n",
    "columns = ['initializer', 'regularization', 'activation_function', 'dropout']\n",
    "var_X = df['best_valid_acc'].var()\n",
    "var_x = {}\n",
    "for param in columns:\n",
    "    df_copy = df.copy()\n",
    "    var_x[param] = float(np.var(df.groupby(param)['best_valid_acc'].mean()) / var_X) # type: ignore\n",
    "\n",
    "#Forgot to add the learning rate to the csv but we can still derive its variance\n",
    "var_x['lr'] = float(np.var(df.groupby(columns)['best_valid_acc'].mean()) / var_X) # type: ignore\n",
    "print('vanilla NN: ', var_x)\n",
    "\n",
    "df = pd.read_csv('Configuration_performance_file_cnn.csv', index_col=0)\n",
    "columns = ['initializer', 'regularization', 'activation_function', 'dropout', 'optimizer']\n",
    "var_X = df['best_valid_acc'].var()\n",
    "var_x = {}\n",
    "for param in columns:\n",
    "    df_copy = df.copy()\n",
    "    var_x[param] = float(np.var(df.groupby(param)['best_valid_acc'].mean().to_numpy()) / var_X) # type: ignore\n",
    "\n",
    "print('CNN: ', var_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78f8a0c",
   "metadata": {},
   "source": [
    "It is clear form this that the initializer and regularization barely had any effect on the performance of the vanill NN. For the regularization this is to be expected as our models are not that deep and the effect of exploding gradients does not really occur yet (Look for reference). For the initializers it is not yet clear why this has no effect. The full effect probably only occurs when the network is larger aswell but there is not yet a derivation or source found that supports this.\n",
    "\n",
    "However, for the next part we will also the CNN on the most important hyperparameters (HP). These params or activation funcitons, dropouts and learning rates. We assumed that the importance of the HPs would also count for the CNN and not only the vanilla NN and this leads us to think that we only need to train the CNN varying those HPs. This is also done to reduce computation time as running the CNN on all different Hyperparameter configurations (HPC) as those for the vanilla NN would approx take 50 hours which seems a bit extreme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c84a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_funcs = ['relu', 'tanh', 'sigmoid'] \n",
    "dropouts = [0, 0.3, 0.5]\n",
    "learning_rates = [0.005, 0.01, 0.015, 0.02]\n",
    "\n",
    "len_loops = len(activation_funcs) * len(dropouts) * len(learning_rates)\n",
    "i = 0\n",
    "\n",
    "final = pd.DataFrame([], columns=['optimizer', 'initializer', 'regularization', 'activation_function', 'best_valid_acc', 'last_valid_acc', 'last_train_acc', 'model'])\n",
    "for lr in learning_rates:\n",
    "    for active in activation_funcs:\n",
    "        for drops in dropouts:\n",
    "            if (i % 10) == 0:\n",
    "                print(f\"{i}/{len_loops}\")\n",
    "            i += 1\n",
    "            optimiz = AdamW(learning_rate=lr/10)\n",
    "            model = CNN(initialization=RandomNormal(), activation=active, optimizer=optimiz, regularization=None, dropout=drops)\n",
    "            hist = model.fit_model(X_train, y_train, epochs=15, validation_data=(X_valid, y_valid))\n",
    "            max_val = max(hist.history['val_accuracy'])\n",
    "            last_val = hist.history['val_accuracy'][-1]\n",
    "            last_train = hist.history['accuracy'][-1]\n",
    "\n",
    "            setup_dict = pd.DataFrame([{'optimizer': 'AdamW', \n",
    "                'initializer': 'RandomNormal',\n",
    "                'regularization': None,\n",
    "                'activation_function': active,\n",
    "                'dropout': str(drops),\n",
    "                'best_valid_acc': max_val,\n",
    "                'last_valid_acc': last_val,\n",
    "                'last_train_acc': last_train,\n",
    "                'model': 'CNN'}])\n",
    "                \n",
    "            final = pd.concat((final, setup_dict), ignore_index=True)\n",
    "\n",
    "final.to_csv('Configuration_performance_file_cnn_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b2bdd8",
   "metadata": {},
   "source": [
    "### Best configurations\n",
    "\n",
    "We will first make a boxplot of the CNN performances vs the Vanilla NN to see what works best. Afte that we will take the top-3 configurations to also train on the CIFAR-10 dataset. But before we train those networks on the CIFAR-10, we'll also test the performances on the test set to see if they actually generalized well. \n",
    "\n",
    "Enough talking, lets get into it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "688bac26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGdCAYAAAD60sxaAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAO11JREFUeJzt3XtclGX+//H3iBwGUdRU1ETR1CCPgYeELDUDzXPbim1qGppmmxJuJesp7UBqoZVBHjIzLe3gdlpLWS3TyNAx20xMKwlTyNQEVxQQ5/eHD+f3nUCbW2fkcL+ej8c8lrnmuq/7c/dYhrfXfd/XbbHb7XYBAACYSLXyLgAAAOBqIwABAADTIQABAADTIQABAADTIQABAADTIQABAADTIQABAADTIQABAADTqV7eBVRE586d0+HDh1WzZk1ZLJbyLgcAALjAbrfr5MmTaty4sapVu/QcDwGoDIcPH1ZwcHB5lwEAAC7DwYMH1aRJk0v2IQCVoWbNmpLO/wesVatWOVcDAABckZ+fr+DgYMff8UshAJXhwmmvWrVqEYAAAKhkXLl8hYugAQCA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6ZR7AEpJSVHz5s3l5+eniIgIbdmy5ZL9X3rpJYWFhclqter666/XihUrnD5fu3atOnXqpNq1a6tGjRrq2LGjXn/9dU8eAgAAqGTKdSXoNWvWKD4+XikpKYqKitKiRYvUt29f7dmzR02bNi3VPzU1VYmJiVqyZIk6d+6sjIwMjR07VnXq1NGAAQMkSXXr1tXUqVMVGhoqHx8fffTRRxo9erQaNGigmJiYq32IAACgArLY7XZ7ee28a9euCg8PV2pqqqMtLCxMgwcPVlJSUqn+kZGRioqK0rx58xxt8fHx2rFjh7Zu3XrR/YSHh6tfv3564oknXKorPz9fgYGBysvL41EYAABUEkb+fpfbKbCioiLZbDZFR0c7tUdHRys9Pb3MbQoLC+Xn5+fUZrValZGRoeLi4lL97Xa7Nm7cqO+//1633HKL+4oHAACVWrmdAjt69KhKSkoUFBTk1B4UFKTc3Nwyt4mJidHSpUs1ePBghYeHy2azadmyZSouLtbRo0fVqFEjSVJeXp6uvfZaFRYWysvLSykpKbr99tsvWkthYaEKCwsd7/Pz891whKgICgoKtHfv3kv2OX36tLKyshQSEiKr1fqnY4aGhsrf399dJQIAykG5Pw3+j09stdvtF32K6/Tp05Wbm6ubbrpJdrtdQUFBGjVqlObOnSsvLy9Hv5o1a2rXrl363//+p40bNyohIUEtWrRQjx49yhw3KSlJs2bNctsxoeLYu3evIiIi3DqmzWZTeHi4W8cEAFxd5XYNUFFRkfz9/fX2229ryJAhjvZJkyZp165d2rx580W3LS4u1q+//qpGjRpp8eLFeuyxx3TixAlVq1b2Gb0xY8bo4MGDWr9+fZmflzUDFBwczDVAVYArM0CZmZkaPny4Vq5cqbCwsD8dkxkgAKiYjFwDVG4zQD4+PoqIiFBaWppTAEpLS9OgQYMuua23t7eaNGkiSVq9erX69+9/0fAjnZ9V+r8B5498fX3l6+tr8AhQGfj7+7s8WxMWFsbMDgCYRLmeAktISNCIESPUqVMndevWTYsXL1Z2drbGjx8vSUpMTNShQ4cca/3s27dPGRkZ6tq1q37//XclJydr9+7deu211xxjJiUlqVOnTrruuutUVFSkdevWacWKFU53mgEAAHMr1wAUGxurY8eOafbs2crJyVHbtm21bt06NWvWTJKUk5Oj7OxsR/+SkhI999xz+v777+Xt7a2ePXsqPT1dISEhjj6nTp3ShAkT9Msvv8hqtSo0NFQrV65UbGzs1T48AABQQZXrOkAVFesAmcvOnTsVERHBxc0AUMlVinWAAAAAygsBCAAAmA4BCAAAmA4BCAAAmA4BCAAAmA4BCAAAmA4BCAAAmA4BCAAAmA4BCAAAmA4BCAAAmA4BCAAAmA4BCAAAmA4BCAAAmA4BCAAAmA4BCAAAmA4BCAAAmA4BCAAAmE718i4AuBL79+/XyZMnr2iMzMxMp/+9UjVr1lSrVq3cMhYAwDMIQKi09u/fr9atW7ttvOHDh7ttrH379hGCAKACIwCh0row87Ny5UqFhYVd9jinT59WVlaWQkJCZLVar6imzMxMDR8+/IpnpQAAnkUAQqUXFham8PDwKxojKirKTdUAACoDLoIGAACmQwACAACmQwACAACmQwACAACmQwACAACmQwACAACmQwACAACmQwACAACmQwACAACmQwACAACmQwACAACmQwACAACmQwACAACmQwACAACmQwACAACmQwACAACmQwACAACmQwACAACmQwACAACmQwACAACmQwACAACmU+4BKCUlRc2bN5efn58iIiK0ZcuWS/Z/6aWXFBYWJqvVquuvv14rVqxw+nzJkiXq3r276tSpozp16qh3797KyMjw5CEAAIBKplwD0Jo1axQfH6+pU6fq66+/Vvfu3dW3b19lZ2eX2T81NVWJiYl6/PHH9d1332nWrFl68MEH9eGHHzr6fPbZZ7r77rv16aef6ssvv1TTpk0VHR2tQ4cOXa3DAgAAFZzFbrfby2vnXbt2VXh4uFJTUx1tYWFhGjx4sJKSkkr1j4yMVFRUlObNm+doi4+P144dO7R169Yy91FSUqI6depo4cKFGjlypEt15efnKzAwUHl5eapVq5bBo8LVsnPnTkVERMhmsyk8PLy8y5FUMWsCALMw8ve73GaAioqKZLPZFB0d7dQeHR2t9PT0MrcpLCyUn5+fU5vValVGRoaKi4vL3KagoEDFxcWqW7fuRWspLCxUfn6+0wsAAFRd5RaAjh49qpKSEgUFBTm1BwUFKTc3t8xtYmJitHTpUtlsNtntdu3YsUPLli1TcXGxjh49WuY2U6ZM0bXXXqvevXtftJakpCQFBgY6XsHBwZd/YAAAoMIr94ugLRaL03u73V6q7YLp06erb9++uummm+Tt7a1BgwZp1KhRkiQvL69S/efOnas333xTa9euLTVz9H8lJiYqLy/P8Tp48ODlHxAAAKjwyi0A1atXT15eXqVme44cOVJqVugCq9WqZcuWqaCgQFlZWcrOzlZISIhq1qypevXqOfV99tln9fTTT2vDhg1q3779JWvx9fVVrVq1nF4AAKDqKrcA5OPjo4iICKWlpTm1p6WlKTIy8pLbent7q0mTJvLy8tLq1avVv39/Vav2/w9l3rx5euKJJ/TJJ5+oU6dOHqkfAABUXtXLc+cJCQkaMWKEOnXqpG7dumnx4sXKzs7W+PHjJZ0/NXXo0CHHWj/79u1TRkaGunbtqt9//13JycnavXu3XnvtNceYc+fO1fTp0/XGG28oJCTEMcMUEBCggICAq3+Q8KiGARZZT+yTDpf72VxJkvXEPjUMKPsULgCg4ijXABQbG6tjx45p9uzZysnJUdu2bbVu3To1a9ZMkpSTk+O0JlBJSYmee+45ff/99/L29lbPnj2Vnp6ukJAQR5+UlBQVFRXprrvuctrXzJkz9fjjj1+Nw8JVNC7CR2Gfj5M+L+9KzgvT+ZoAABVbua4DVFGxDlDlsHPnTvW7tZM2vf+GwkJDy7scSVLm3r3qNehv+vfmHawDBABXmZG/3+U6AwRcqdz/2XW6dmupccfyLkWSdDr3nHL/x78pAKCiqxgXTgAAAFxFBCAAAGA6BCAAAGA6BCAAAGA6BCAAAGA6BCAAAGA6BCAAAGA6BCAAAGA6BCAAAGA6BCAAAGA6BCAAAGA6hgPQqVOnPFEHAADAVWM4AAUFBem+++7T1q1bPVEPAACAxxkOQG+++aby8vJ02223qXXr1nrmmWd0+PBhT9QGAADgEYYD0IABA/Tuu+/q8OHDeuCBB/Tmm2+qWbNm6t+/v9auXauzZ896ok4AAAC3ueyLoK+55ho9/PDD+uabb5ScnKz//Oc/uuuuu9S4cWPNmDFDBQUF7qwTAADAbapf7oa5ublasWKFXn31VWVnZ+uuu+5SXFycDh8+rGeeeUbbtm3Thg0b3FkrAACAWxgOQGvXrtWrr76q9evX64YbbtCDDz6o4cOHq3bt2o4+HTt21I033ujOOgEAANzGcAAaPXq0hg0bpi+++EKdO3cus0+LFi00derUKy4OAADAEwwHoJycHPn7+1+yj9Vq1cyZMy+7KAAAAE8yfBH0Z599pvXr15dqX79+vT7++GO3FAUAAOBJhgPQlClTVFJSUqrdbrdrypQpbikKAADAkwwHoP379+uGG24o1R4aGqoffvjBLUUBAAB4kuEAFBgYqJ9++qlU+w8//KAaNWq4pSgAAABPMhyABg4cqPj4eP3444+Oth9++EGTJ0/WwIED3VocAACAJxgOQPPmzVONGjUUGhqq5s2bq3nz5goLC9M111yjZ5991hM1AgAAuJXh2+ADAwOVnp6utLQ0ffPNN7JarWrfvr1uueUWT9QHAADgdpf1KAyLxaLo6GhFR0e7ux4AAACPu6wAdOrUKW3evFnZ2dkqKipy+mzixIluKQwAAMBTDAegr7/+WnfccYcKCgp06tQp1a1bV0ePHpW/v78aNGhAAAIAABWe4YugH374YQ0YMEDHjx+X1WrVtm3b9PPPPysiIoKLoAEAQKVgOADt2rVLkydPlpeXl7y8vFRYWKjg4GDNnTtX//znPz1RIwAAgFsZDkDe3t6yWCySpKCgIGVnZ0s6f3fYhZ8BAAAqMsPXAN14443asWOHWrdurZ49e2rGjBk6evSoXn/9dbVr184TNQIAALiV4Rmgp59+Wo0aNZIkPfHEE7rmmmv0wAMP6MiRI1q8eLHbCwQAAHA3QzNAdrtd9evXV5s2bSRJ9evX17p16zxSGAAAgKcYmgGy2+1q1aqVfvnlF0/VAwAA4HGGAlC1atXUqlUrHTt2zFP1AAAAeJzha4Dmzp2rRx55RLt37/ZEPQAAAB5n+C6w4cOHq6CgQB06dJCPj4+sVqvT58ePH3dbcQAAAJ5gOAAtWLDAA2UAAABcPYYD0L333uvWAlJSUjRv3jzl5OSoTZs2WrBggbp3737R/i+99JIWLlyorKwsNW3aVFOnTtXIkSMdn3/33XeaMWOGbDabfv75Z82fP1/x8fFurRkAAFRuhgPQn6323LRpU5fHWrNmjeLj45WSkqKoqCgtWrRIffv21Z49e8ocJzU1VYmJiVqyZIk6d+6sjIwMjR07VnXq1NGAAQMkSQUFBWrRooX++te/6uGHHzZ2cAAAwBQMB6CQkBDHozDKUlJS4vJYycnJiouL05gxYySdP722fv16paamKikpqVT/119/XePGjVNsbKwkqUWLFtq2bZvmzJnjCECdO3dW586dJUlTpkxxuRYAAGAehgPQ119/7fS+uLhYX3/9tZKTk/XUU0+5PE5RUZFsNlupkBIdHa309PQytyksLJSfn59Tm9VqVUZGhoqLi+Xt7e3y/v84bmFhoeN9fn7+ZY0DAAAqB8MBqEOHDqXaOnXqpMaNG2vevHm68847XRrn6NGjKikpUVBQkFN7UFCQcnNzy9wmJiZGS5cu1eDBgxUeHi6bzaZly5apuLhYR48edTyiw6ikpCTNmjXrsrYFAACVj+F1gC6mdevW2r59u+Ht/ng6zW63X/QU2/Tp09W3b1/ddNNN8vb21qBBgzRq1ChJkpeXl+F9X5CYmKi8vDzH6+DBg5c9FgAAqPgMB6D8/HynV15envbu3avp06erVatWLo9Tr149eXl5lZrtOXLkSKlZoQusVquWLVumgoICZWVlKTs7WyEhIapZs6bq1atn9FAcfH19VatWLacXAACougyfAqtdu3aZszbBwcFavXq1y+P4+PgoIiJCaWlpGjJkiKM9LS1NgwYNuuS23t7eatKkiSRp9erV6t+/v6pVc9tkFgAAqOIMB6BNmzY5BaBq1aqpfv36atmypapXNzZcQkKCRowYoU6dOqlbt25avHixsrOzNX78eEnnT00dOnRIK1askCTt27dPGRkZ6tq1q37//XclJydr9+7deu211xxjFhUVac+ePY6fDx06pF27dikgIEAtW7Y0ergAAKAKMhyAevTo4badx8bG6tixY5o9e7ZycnLUtm1brVu3Ts2aNZMk5eTkOK07VFJSoueee07ff/+9vL291bNnT6WnpyskJMTR5/Dhw7rxxhsd75999lk9++yzuvXWW/XZZ5+5rXYAAFB5GQ5ASUlJCgoK0n333efUvmzZMv3222967LHHDI03YcIETZgwoczPli9f7vQ+LCys1G34fxQSEiK73W6oBgAAYC6GL5xZtGiRQkNDS7W3adNGL7/8sluKAgAA8CTDASg3N7fM9Xbq16+vnJwctxQFAADgSYYDUHBwsL744otS7V988YUaN27slqIAAAA8yfA1QGPGjFF8fLyKi4vVq1cvSdLGjRv16KOPavLkyW4vEAAAwN0MB6BHH31Ux48f14QJE1RUVCRJ8vPz02OPPcbDRwEAQKVgOABZLBbNmTNH06dPV2ZmpqxWq1q1aiVfX19P1AcAAOB2hgNQXl6eSkpKVLduXXXu3NnRfvz4cVWvXp3HSAAAgArP8EXQw4YNK/ORF2+99ZaGDRvmlqIAAAA8yXAA+uqrr9SzZ89S7T169NBXX33llqIAAAA8yXAAKiws1NmzZ0u1FxcX6/Tp024pCgAAwJMMB6DOnTtr8eLFpdpffvllRUREuKUoAAAATzJ8EfRTTz2l3r1765tvvtFtt90m6fw6QNu3b9eGDRvcXiAAAIC7GZ4BioqK0pdffqng4GC99dZb+vDDD9WyZUv997//Vffu3T1RIwAAgFsZngGSpI4dO2rVqlXurgUAAOCquKwAdMHp06dVXFzs1MY6QAAAoKIzfAqsoKBAf//739WgQQMFBASoTp06Ti8AAICKznAAeuSRR7Rp0yalpKTI19dXS5cu1axZs9S4cWOtWLHCEzUCAAC4leFTYB9++KFWrFihHj166L777lP37t3VsmVLNWvWTKtWrdI999zjiToBAADcxvAM0PHjx9W8eXNJ56/3OX78uCTp5ptv1ueff+7e6gAAADzA8AxQixYtlJWVpWbNmumGG27QW2+9pS5duujDDz9U7dq1PVAiAAClFRQUaO/evZfsc/r0aWVlZSkkJERWq/VPxwwNDZW/v7+7SkQFZjgAjR49Wt98841uvfVWJSYmql+/fnrxxRd19uxZJScne6JGAABK2bt3r9ufQGCz2RQeHu7WMVExGQ5ADz/8sOPnnj17au/evdqxY4euu+46dejQwa3FAQBwMaGhobLZbJfsk5mZqeHDh2vlypUKCwtzaUyYwxWtAyRJTZs2VdOmTd1RCwAALvP393d5tiYsLIyZHTgxfBE0AABAZUcAAgAApkMAAgAApkMAAgAApmM4AHl5eenIkSOl2o8dOyYvLy+3FAUAAOBJhgOQ3W4vs72wsFA+Pj5XXBAAAICnuXwb/AsvvCBJslgsWrp0qQICAhyflZSU6PPPP2f9BAAAUCm4HIDmz58v6fwM0Msvv+x0usvHx0chISF6+eWX3V8hAACAm7kcgA4cOCDp/OrPa9euVZ06dTxWFAAAgCcZvgbo008/VZ06dVRUVKTvv/9eZ8+e9URdAAAAHmM4AJ0+fVpxcXHy9/dXmzZtlJ2dLUmaOHGinnnmGbcXCAAA4G6GA9CUKVP0zTff6LPPPpOfn5+jvXfv3lqzZo1biwMAAPAEww9Dfe+997RmzRrddNNNslgsjvYbbrhBP/74o1uLAwAA8ATDM0C//fabGjRoUKr91KlTToEIAACgojIcgDp37qx///vfjvcXQs+SJUvUrVs391UGAADgIYZPgSUlJalPnz7as2ePzp49q+eff17fffedvvzyS23evNkTNQIAALiV4RmgyMhIffHFFyooKNB1112nDRs2KCgoSF9++aUiIiI8USMAAIBbGZ4BkqR27drptddec3ctAAAAV4XhALRz5055e3urXbt2kqT3339fr776qm644QY9/vjjPBAVV01BQYGk8/+fvBKnT59WVlaWQkJCZLVar2iszMzMK9oeAHCV2A3q1KmT/Z133rHb7Xb7jz/+aPf19bXffffd9pYtW9onTZpkdDj7Sy+9ZA8JCbH7+vraw8PD7Z9//vkl+y9cuNAeGhpq9/Pzs7du3dr+2muvlerzzjvv2MPCwuw+Pj72sLAw+9q1aw3VlJeXZ5dkz8vLM7Qdrq4lS5bYJVXI1759+8r7Pw8Au91us9nskuw2m628S8FVYOTvt+EZoH379qljx46SpLffflu33nqr3njjDX3xxRcaNmyYFixY4PJYa9asUXx8vFJSUhQVFaVFixapb9++2rNnj5o2bVqqf2pqqhITE7VkyRJ17txZGRkZGjt2rOrUqaMBAwZIkr788kvFxsbqiSee0JAhQ/Svf/1LQ4cO1datW9W1a1ejh4sKbPDgwZKk0NBQ+fv7X/Y4mZmZGj58uFauXKmwsLArrqtmzZpq1arVFY8DAPAci91utxvZoFatWrLZbGrVqpVuv/129e/fX5MmTVJ2drauv/56nT592uWxunbtqvDwcKWmpjrawsLCNHjwYCUlJZXqHxkZqaioKM2bN8/RFh8frx07dmjr1q2SpNjYWOXn5+vjjz929OnTp4/q1KmjN99806W68vPzFRgYqLy8PNWqVcvl40HltHPnTkVERMhmsyk8PLy8ywHgRvx+m4uRv9+G7wLr1KmTnnzySb3++uvavHmz+vXrJ+n80+KDgoJcHqeoqEg2m03R0dFO7dHR0UpPTy9zm8LCQqfHb0iS1WpVRkaGiouLJZ2fAfrjmDExMRcd88K4+fn5Ti8AAFB1GQ5ACxYs0M6dO/X3v/9dU6dOVcuWLSVJ77zzjiIjI10e5+jRoyopKSkVmoKCgpSbm1vmNjExMVq6dKlsNpvsdrt27NihZcuWqbi4WEePHpUk5ebmGhpTOr+2UWBgoOMVHBzs8nEAAIDKx/A1QO3bt9e3335bqn3evHny8vIyXMAfH59ht9sv+kiN6dOnKzc3VzfddJPsdruCgoI0atQozZ0712nfRsaUpMTERCUkJDje5+fnE4IAAKjCDM8AXYyfn5+8vb1d7l+vXj15eXmVmpk5cuTIRU+lWa1WLVu2TAUFBcrKylJ2drZCQkJUs2ZN1atXT5LUsGFDQ2NKkq+vr2rVquX0AgAAVZfbApBRPj4+ioiIUFpamlN7Wlran55K8/b2VpMmTeTl5aXVq1erf//+qlbt/KF069at1JgbNmwwdHoOAABUbZe1ErS7JCQkaMSIEerUqZO6deumxYsXKzs7W+PHj5d0/tTUoUOHtGLFCknnb8HPyMhQ165d9fvvvys5OVm7d+92WpV60qRJuuWWWzRnzhwNGjRI77//vv7zn/847hIDAFQO+/fv18mTJ69ojAuLk7prkVKWuag6yjUAxcbG6tixY5o9e7ZycnLUtm1brVu3Ts2aNZMk5eTkKDs729G/pKREzz33nL7//nt5e3urZ8+eSk9PV0hIiKNPZGSkVq9erWnTpmn69Om67rrrtGbNGtYAAoBKZP/+/WrdurXbxhs+fLjbxtq3bx8hqAowvA6QGbAOkLmwTghQ8Vz4vbzSBUrd/aib4cOH811RgRn5+214BqikpETLly/Xxo0bdeTIEZ07d87p802bNhkdEgCAMoWFhV1x2IiKinJTNahKDAegSZMmafny5erXr5/atm17ydvLAQAAKiLDAWj16tV66623dMcdd3iiHgAAAI8zfBu8j4+PY/VnAACAyshwAJo8ebKef/55ce00AACorAyfAtu6das+/fRTffzxx2rTpk2p1Z/Xrl3rtuIAAAA8wXAAql27toYMGeKJWgAAAK4KwwHo1Vdf9UQdAAAAV81lrwT922+/6fvvv5fFYlHr1q1Vv359d9YFAADgMYYvgj516pTuu+8+NWrUSLfccou6d++uxo0bKy4uTgUFBZ6oEQAAwK0MB6CEhARt3rxZH374oU6cOKETJ07o/fff1+bNmzV58mRP1AgAAOBWhk+Bvfvuu3rnnXfUo0cPR9sdd9whq9WqoUOHKjU11Z31AQAAuJ3hGaCCggIFBQWVam/QoAGnwAAAQKVgOAB169ZNM2fO1JkzZxxtp0+f1qxZs9StWze3FgcAAOAJhk+BPf/88+rTp4+aNGmiDh06yGKxaNeuXfLz89P69es9USMAAIBbGQ5Abdu21f79+7Vy5Urt3btXdrtdw4YN0z333COr1eqJGgEAANzqstYBslqtGjt2rLtrAQAAuCpcCkAffPCB+vbtK29vb33wwQeX7Dtw4EC3FAYAAOApLgWgwYMHKzc3Vw0aNNDgwYMv2s9isaikpMRdtQEATKxhgEXWE/ukw4bv1/EI64l9ahhgKe8y4CYuBaBz586V+TMAAJ4yLsJHYZ+Pkz4v70rOC9P5mlA1GL4GaMWKFYqNjZWvr69Te1FRkVavXq2RI0e6rTgAgHktshUpdsZyhYWGlncpkqTMvXu16Lm/iQs9qgbDAWj06NHq06ePGjRo4NR+8uRJjR49mgAEAHCL3P/Zdbp2a6lxx/IuRZJ0Oveccv9nL+8y4CaGT6za7XZZLKXPgf7yyy8KDAx0S1EAAACe5PIM0I033iiLxSKLxaLbbrtN1av//01LSkp04MAB9enTxyNFAgAAuJPLAejC3V+7du1STEyMAgICHJ/5+PgoJCREf/nLX9xeIAAAgLu5HIBmzpwpSQoJCVFsbKz8/Pw8VhQAAIAnGb4I+t577/VEHQAAAFeN4QBUUlKi+fPn66233lJ2draKioqcPj9+/LjbigMAAPAEw3eBzZo1S8nJyRo6dKjy8vKUkJCgO++8U9WqVdPjjz/ugRIBAADcy3AAWrVqlZYsWaJ//OMfql69uu6++24tXbpUM2bM0LZt2zxRIwAAgFsZDkC5ublq166dJCkgIEB5eXmSpP79++vf//63e6sDAADwAMMBqEmTJsrJyZEktWzZUhs2bJAkbd++vdTjMQAAACoiwwFoyJAh2rhxoyRp0qRJmj59ulq1aqWRI0fqvvvuc3uBAAAA7mb4LrBnnnnG8fNdd92lJk2aKD09XS1bttTAgTwiDgAAVHyGA9Af3XTTTbrpppvcUQsAAMBV4VIA+uCDD1wekFkgAABQ0bkUgC48B+wCi8Uiu91eqk06v1AiAABARebSRdDnzp1zvDZs2KCOHTvq448/1okTJ5SXl6ePP/5Y4eHh+uSTTzxdLwAAwBUzfA1QfHy8Xn75Zd18882OtpiYGPn7++v+++9XZmamWwsEAABwN8O3wf/4448KDAws1R4YGKisrCx31AQAAOBRhgNQ586dFR8f71gMUTq/OvTkyZPVpUsXtxYHAADgCYYD0LJly3TkyBE1a9ZMLVu2VMuWLdW0aVPl5OTolVde8USNAAAAbmU4ALVs2VL//e9/9dFHH2nixIl66KGH9O9//1vffvutWrZsabiAlJQUNW/eXH5+foqIiNCWLVsu2X/VqlXq0KGD/P391ahRI40ePVrHjh1zfF5cXKzZs2fruuuuk5+fnzp06MDF2QAAwMllLYRosVgUHR2t6OjoK9r5mjVrFB8fr5SUFEVFRWnRokXq27ev9uzZo6ZNm5bqv3XrVo0cOVLz58/XgAEDdOjQIY0fP15jxozRv/71L0nStGnTtHLlSi1ZskShoaFav369hgwZovT0dN14441XVC8AAKgaXApAL7zwgu6//375+fnphRdeuGTfiRMnurzz5ORkxcXFacyYMZKkBQsWaP369UpNTVVSUlKp/tu2bVNISIhjH82bN9e4ceM0d+5cR5/XX39dU6dO1R133CFJeuCBB7R+/Xo999xzWrlypcu1AQCAqsulADR//nzdc8898vPz0/z58y/az2KxuByAioqKZLPZNGXKFKf26Ohopaenl7lNZGSkpk6dqnXr1qlv3746cuSI3nnnHfXr18/Rp7CwUH5+fk7bWa1Wbd269aK1FBYWqrCw0PE+Pz/fpWMAAACVk0sB6MCBA2X+fCWOHj2qkpISBQUFObUHBQUpNze3zG0iIyO1atUqxcbG6syZMzp79qwGDhyoF1980dEnJiZGycnJuuWWW3Tddddp48aNev/99y+5QnVSUpJmzZrlluMCAAAVn+GLoN3twiM0LrDb7aXaLtizZ48mTpyoGTNmyGaz6ZNPPtGBAwc0fvx4R5/nn39erVq1UmhoqHx8fPT3v/9do0ePlpeX10VrSExMVF5enuN18OBB9xwcAACokFyaAUpISHB5wOTkZJf61atXT15eXqVme44cOVJqVuiCpKQkRUVF6ZFHHpEktW/fXjVq1FD37t315JNPqlGjRqpfv77ee+89nTlzRseOHVPjxo01ZcoUNW/e/KK1+Pr6ytfX18UjBAAAlZ1LAejrr792abCLzdyUxcfHRxEREUpLS9OQIUMc7WlpaRo0aFCZ2xQUFKh6deeSL8zs/PHhrH5+frr22mtVXFysd999V0OHDnW5NgAAULW5FIA+/fRTj+w8ISFBI0aMUKdOndStWzctXrxY2dnZjlNaiYmJOnTokFasWCFJGjBggMaOHavU1FTFxMQoJydH8fHx6tKlixo3bixJ+uqrr3To0CF17NhRhw4d0uOPP65z587p0Ucf9cgxAACAyuey1gFyl9jYWB07dkyzZ89WTk6O2rZtq3Xr1qlZs2aSpJycHGVnZzv6jxo1SidPntTChQs1efJk1a5dW7169dKcOXMcfc6cOaNp06bpp59+UkBAgO644w69/vrrql279tU+PAAAUEFdVgDavn273n77bWVnZ6uoqMjps7Vr1xoaa8KECZowYUKZny1fvrxU20MPPaSHHnroouPdeuut2rNnj6EaAACAuRi+C2z16tWKiorSnj179K9//UvFxcXas2ePNm3aVOZT4gEAACoawwHo6aef1vz58/XRRx/Jx8dHzz//vDIzMzV06NAyH18BAABQ0RgOQD/++KNj5WVfX1+dOnVKFotFDz/8sBYvXuz2AgEAANzNcACqW7euTp48KUm69tprtXv3bknSiRMnVFBQ4N7qAAAAPMDwRdDdu3dXWlqa2rVrp6FDh2rSpEnatGmT0tLSdNttt3miRgAAALcyHIAWLlyoM2fOSDq/To+3t7e2bt2qO++8U9OnT3d7gQAAAO5mOADVrVvX8XO1atX06KOPssggAACoVAxfA9SzZ0+98sorysvL80Q9AAAAHmc4ALVr107Tpk1Tw4YN9Ze//EXvvfdeqcUQAQAAKjLDAeiFF17QoUOH9P7776tmzZq699571bBhQ91///3avHmzJ2oEAABwK8MBSDp/7U90dLSWL1+uX3/9VYsWLVJGRoZ69erl7voAAADc7ooehpqbm6vVq1dr5cqV+u9//6vOnTu7qy4AAACPMTwDlJ+fr1dffVW33367goODlZqaqgEDBmjfvn366quvPFEjAACAWxmeAQoKClKdOnU0dOhQPf3008z6AACASsdwAHr//ffVu3dvVat2WZcPAQAAlDvDASg6OtoTdQAAAFw1TOMAAADTuaK7wAAA8ISCggJJ0s6dO69onNOnTysrK0shISGyWq1XNFZmZuYVbY+KhQAEAKhw9u7dK0kaO3ZsOVdSWs2aNcu7BLiB4QC0YsUKxcbGytfX16m9qKhIq1ev1siRI91WHADAnAYPHixJCg0Nlb+//2WPk5mZqeHDh2vlypUKCwu74rpq1qypVq1aXfE4KH+GA9Do0aPVp08fNWjQwKn95MmTGj16NAEIAHDF6tWrpzFjxrhtvLCwMIWHh7ttPFR+hi+Cttvtslgspdp/+eUXBQYGuqUoAAAAT3J5BujGG2+UxWKRxWLRbbfdpurV//+mJSUlOnDggPr06eORIgEAANzJ5QB04Xzsrl27FBMTo4CAAMdnPj4+CgkJ0V/+8he3FwgAAOBuLgegmTNnSpJCQkI0bNiwUhdBAwAAVBaGrwHq1auXfvvtN8f7jIwMxcfHa/HixW4tDAAAwFMMB6C//e1v+vTTTyVJubm56t27tzIyMvTPf/5Ts2fPdnuBAAAA7mY4AO3evVtdunSRJL311ltq166d0tPT9cYbb2j58uXurg8AAMDtDAeg4uJix/U///nPfzRw4EBJ5xerysnJcW91AAAAHmA4ALVp00Yvv/yytmzZorS0NMet74cPH9Y111zj9gIBAADczXAAmjNnjhYtWqQePXro7rvvVocOHSRJH3zwgePUGAAAQEVm+FEYPXr00NGjR5Wfn686deo42u+///4rel4LAADA1WJ4Bkg6/zgMm82mRYsW6eTJk5LOL4ZIAAIAAJWB4Rmgn3/+WX369FF2drYKCwt1++23q2bNmpo7d67OnDmjl19+2RN1AgAAuI3hGaBJkyapU6dO+v3332W1Wh3tQ4YM0caNG91aHAAAgCcYngHaunWrvvjiC/n4+Di1N2vWTIcOHXJbYQAAAJ5ieAbo3LlzKikpKdX+yy+/qGbNmm4pCgAAwJMMB6Dbb79dCxYscLy3WCz63//+p5kzZ+qOO+5wZ20AAAAeYfgU2Pz589WzZ0/dcMMNOnPmjP72t79p//79qlevnt58801P1AgAAOBWhgNQ48aNtWvXLq1evVo2m03nzp1TXFyc7rnnHqeLogEAACoqwwFIkqxWq0aPHq3Ro0e7ux4AAACPM3wN0LFjxxw/Hzx4UDNmzNAjjzyizz///LIKSElJUfPmzeXn56eIiAht2bLlkv1XrVqlDh06yN/fX40aNdLo0aOdapKkBQsW6Prrr5fValVwcLAefvhhnTlz5rLqAwAAVY/LAejbb79VSEiIGjRooNDQUO3atUudO3fW/PnztXjxYvXq1UvvvfeeoZ2vWbNG8fHxmjp1qr7++mt1795dffv2VXZ2dpn9t27dqpEjRyouLk7fffed3n77bW3fvl1jxoxx9Fm1apWmTJmimTNnKjMzU6+88orWrFmjxMREQ7UBAICqy+UA9Oijj6pdu3bavHmzevToof79++uOO+5QXl6efv/9d40bN07PPPOMoZ0nJycrLi5OY8aMUVhYmBYsWKDg4GClpqaW2X/btm0KCQnRxIkT1bx5c918880aN26cduzY4ejz5ZdfKioqSn/7298UEhKi6Oho3X333U59AACAubkcgLZv366nnnpKN998s5599lkdPnxYEyZMULVq1VStWjU99NBD2rt3r8s7Lioqks1mU3R0tFN7dHS00tPTy9wmMjJSv/zyi9atWye73a5ff/1V77zzjvr16+foc/PNN8tmsykjI0OS9NNPP2ndunVOff6osLBQ+fn5Ti8AAFB1uXwR9PHjx9WwYUNJUkBAgGrUqKG6des6Pq9Tp47jwaiuOHr0qEpKShQUFOTUHhQUpNzc3DK3iYyM1KpVqxQbG6szZ87o7NmzGjhwoF588UVHn2HDhum3337TzTffLLvdrrNnz+qBBx7QlClTLlpLUlKSZs2a5XLtAACgcjN0EbTFYrnk+8vxxzHsdvtFx92zZ48mTpyoGTNmyGaz6ZNPPtGBAwc0fvx4R5/PPvtMTz31lFJSUrRz506tXbtWH330kZ544omL1pCYmKi8vDzH6+DBg1d8XAAAoOIydBv8qFGj5OvrK0k6c+aMxo8frxo1akg6fxrJiHr16snLy6vUbM+RI0dKzQpdkJSUpKioKD3yyCOSpPbt26tGjRrq3r27nnzySTVq1EjTp0/XiBEjHBdGt2vXTqdOndL999+vqVOnqlq10pnP19fXcVwAAKDqczkA3XvvvU7vhw8fXqrPyJEjXd6xj4+PIiIilJaWpiFDhjja09LSNGjQoDK3KSgoUPXqziV7eXlJOj9zdKHPH0OOl5eX7Ha7ow8AADA3lwPQq6++6vadJyQkaMSIEerUqZO6deumxYsXKzs723FKKzExUYcOHdKKFSskSQMGDNDYsWOVmpqqmJgY5eTkKD4+Xl26dFHjxo0dfZKTk3XjjTeqa9eu+uGHHzR9+nQNHDjQEZYAAIC5XdZK0O4SGxurY8eOafbs2crJyVHbtm21bt06NWvWTJKUk5PjtCbQqFGjdPLkSS1cuFCTJ09W7dq11atXL82ZM8fRZ9q0abJYLJo2bZoOHTqk+vXra8CAAXrqqaeu+vEBAICKyWLnvFAp+fn5CgwMVF5enmrVqlXe5cDDdu7cqYiICNlsNoWHh5d3OQDciN9vczHy99vwozAAAAAqOwIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwnXIPQCkpKWrevLn8/PwUERGhLVu2XLL/qlWr1KFDB/n7+6tRo0YaPXq0jh075vi8R48eslgspV79+vXz9KEAAIBKolwD0Jo1axQfH6+pU6fq66+/Vvfu3dW3b19lZ2eX2X/r1q0aOXKk4uLi9N133+ntt9/W9u3bNWbMGEeftWvXKicnx/HavXu3vLy89Ne//vVqHRYAAKjgyjUAJScnKy4uTmPGjFFYWJgWLFig4OBgpaamltl/27ZtCgkJ0cSJE9W8eXPdfPPNGjdunHbs2OHoU7duXTVs2NDxSktLk7+/PwEIAAA4lFsAKioqks1mU3R0tFN7dHS00tPTy9wmMjJSv/zyi9atWye73a5ff/1V77zzziVPb73yyisaNmyYatSocdE+hYWFys/Pd3oBAICqq9wC0NGjR1VSUqKgoCCn9qCgIOXm5pa5TWRkpFatWqXY2Fj5+PioYcOGql27tl588cUy+2dkZGj37t1Op8jKkpSUpMDAQMcrODj48g4KAABUCuV+EbTFYnF6b7fbS7VdsGfPHk2cOFEzZsyQzWbTJ598ogMHDmj8+PFl9n/llVfUtm1bdenS5ZI1JCYmKi8vz/E6ePDg5R0MAACoFKqX147r1asnLy+vUrM9R44cKTUrdEFSUpKioqL0yCOPSJLat2+vGjVqqHv37nryySfVqFEjR9+CggKtXr1as2fP/tNafH195evrewVHAwAAKpNymwHy8fFRRESE0tLSnNrT0tIUGRlZ5jYFBQWqVs25ZC8vL0nnZ47+r7feekuFhYUaPny4G6sGAABVQbmeAktISNDSpUu1bNkyZWZm6uGHH1Z2drbjlFZiYqJGjhzp6D9gwACtXbtWqamp+umnn/TFF19o4sSJ6tKlixo3buw09iuvvKLBgwfrmmuuuarHBAAAKr5yOwUmSbGxsTp27Jhmz56tnJwctW3bVuvWrVOzZs0kSTk5OU5rAo0aNUonT57UwoULNXnyZNWuXVu9evXSnDlznMbdt2+ftm7dqg0bNlzV4wEAAJWDxf7Hc0dQfn6+AgMDlZeXp1q1apV3OfCwnTt3KiIiQjabTeHh4eVdDgA34vfbXIz8/S73u8AAAACuNgIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwnXJ9FAbgaQUFBdq7d+8l+2RmZjr9758JDQ2Vv7//FdcGACg/BCBUaXv37lVERIRLfYcPH+5SP5bUB4DKjwCEKi00NFQ2m+2SfU6fPq2srCyFhITIarW6NCYAoHIjAKFK8/f3d2m2Jioq6ipUA8CdOMWNK0EAAgBUSpzixpUgAAEAKiVOceNKWOx2u728i6ho8vPzFRgYqLy8PNWqVau8ywEAAC4w8vebdYAAAIDpEIAAAIDpEIAAAIDpEIAAAIDpEIAAAIDpEIAAAIDpEIAAAIDpEIAAAIDpEIAAAIDpEIAAAIDpEIAAAIDpEIAAAIDpEIAAAIDpVC/vAioiu90u6fxTZQEAQOVw4e/2hb/jl0IAKsPJkyclScHBweVcCQAAMOrkyZMKDAy8ZB+L3ZWYZDLnzp3T4cOHVbNmTVkslvIuBx6Wn5+v4OBgHTx4ULVq1SrvcgC4Eb/f5mK323Xy5Ek1btxY1apd+iofZoDKUK1aNTVp0qS8y8BVVqtWLb4ggSqK32/z+LOZnwu4CBoAAJgOAQgAAJgOAQim5+vrq5kzZ8rX17e8SwHgZvx+42K4CBoAAJgOM0AAAMB0CEAAAMB0CEAAAMB0CEAAgErNYrHovffekyRlZWXJYrFo165dkqTPPvtMFotFJ06cKLf6UDERgFAl5ebm6qGHHlKLFi3k6+ur4OBgDRgwQBs3bpQkhYSEyGKxaNu2bU7bxcfHq0ePHo73jz/+uCwWi8aPH+/Ub9euXbJYLMrKyvL0oQCV0oABA9S7d+8yP/vyyy9lsVi0c+dOt+wrJydHffv2dctYZbkQotq2bauSkhKnz2rXrq3ly5c73rv63YLyRwBClZOVlaWIiAht2rRJc+fO1bfffqtPPvlEPXv21IMPPujo5+fnp8cee+xPx/Pz89Mrr7yiffv2ebJsoEqJi4vTpk2b9PPPP5f6bNmyZerYsaPCw8Pdsq+GDRteldvcf/zxR61YseJP+7n63YLyRQBClTNhwgRZLBZlZGTorrvuUuvWrdWmTRslJCQ4/ats3Lhx2rZtm9atW3fJ8a6//nr17NlT06ZN83TpQJXRv39/NWjQwGl2RJIKCgq0Zs0axcXF6dixY7r77rvVpEkT+fv7q127dnrzzTed+vfo0UMTJ07Uo48+qrp166phw4Z6/PHHnfr831Ngf8aVfV7MQw89pJkzZ+rMmTOX7OfqdwvKFwEIVcrx48f1ySef6MEHH1SNGjVKfV67dm3HzyEhIRo/frwSExN17ty5S477zDPP6N1339X27dvdXTJQJVWvXl0jR47U8uXL9X+Xm3v77bdVVFSke+65R2fOnFFERIQ++ugj7d69W/fff79GjBihr776ymms1157TTVq1NBXX32luXPnavbs2UpLS7usulzdZ1ni4+N19uxZLVy48JL9jHy3oPwQgFCl/PDDD7Lb7QoNDXWp/7Rp03TgwAGtWrXqkv3Cw8M1dOhQTZkyxR1lAqZw3333KSsrS5999pmjbdmyZbrzzjtVp04dXXvttfrHP/6hjh07qkWLFnrooYcUExOjt99+22mc9u3ba+bMmWrVqpVGjhypTp06Oa7nM8rVfZbF399fM2fOVFJSkvLy8i7Z19XvFpQfAhCqlAv/0rRYLC71r1+/vv7xj39oxowZKioqumTfJ598Ulu2bNGGDRuuuE7ADEJDQxUZGally5ZJOn8NzZYtW3TfffdJkkpKSvTUU0+pffv2uuaaaxQQEKANGzYoOzvbaZz27ds7vW/UqJGOHDlyWTW5us+LiYuLU7169TRnzpxL9jPy3YLyQQBCldKqVStZLBZlZma6vE1CQoJOnz6tlJSUS/a77rrrNHbsWE2ZMkU8QQZwTVxcnN59913l5+fr1VdfVbNmzXTbbbdJkp577jnNnz9fjz76qDZt2qRdu3YpJiamVGDw9vZ2em+xWC771JKr+7yY6tWr68knn9Tzzz+vw4cPX7Kvq98tKB8EIFQpdevWVUxMjF566SWdOnWq1OdlrQUSEBCg6dOn66mnnlJ+fv4lx58xY4b27dun1atXu6tkoEobOnSovLy89MYbb+i1117T6NGjHTO0W7Zs0aBBgzR8+HB16NBBLVq00P79+z1ajzv2+de//lVt2rTRrFmzLtnPyHcLrj4CEKqclJQUlZSUqEuXLnr33Xe1f/9+ZWZm6oUXXlC3bt3K3Ob+++9XYGDgn94NEhQUpISEBL3wwgueKB2ocgICAhQbG6t//vOfOnz4sEaNGuX4rGXLlkpLS1N6eroyMzM1btw45ebmerQed+3zmWee0bJly8r8h9b/5ep3C64+AhCqnObNm2vnzp3q2bOnJk+erLZt2+r222/Xxo0blZqaWuY23t7eeuKJJ/709lZJeuSRRxQQEODusoEqKy4uTr///rt69+6tpk2bOtqnT5+u8PBwxcTEqEePHmrYsKEGDx7s0Vrctc9evXqpV69eOnv27CX7GfluwdVlsXMxAwAAMBlmgAAAgOkQgAAAgOkQgAAAgOkQgAAAgOkQgAAAgOkQgAAAgOkQgAAAgOkQgAAAgOkQgAAAgOkQgAAAgOkQgAAAgOkQgAAAgOn8PwgoYpfNYJOYAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_df = pd.read_csv('Configuration_performance_file_cnn_1.csv').iloc[:,1:]\n",
    "vanilla_df = pd.read_csv('Configuration_performance_file.csv').iloc[:,1:]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.boxplot([cnn_df[cnn_df['best_valid_acc'] > 0.86]['best_valid_acc'], vanilla_df[vanilla_df['best_valid_acc'] > 0.86]['best_valid_acc']])\n",
    "ax.set_ylabel('Best validation set accuracy')\n",
    "ax.set_xticklabels(['CNN', 'Vanilla NN'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a003d77",
   "metadata": {},
   "source": [
    "When trimming away the worst performing configurations for both type of networks we can see that the CNN networks outperform the vanill NN, which is to be expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "621627d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   optimizer   initializer regularization activation_function  best_valid_acc  \\\n",
      "2      AdamW  RandomNormal            NaN                relu        0.929833   \n",
      "1      AdamW  RandomNormal            NaN                relu        0.929500   \n",
      "10     AdamW  RandomNormal            NaN                relu        0.929333   \n",
      "\n",
      "    last_valid_acc  last_train_acc model  dropout      lr  \n",
      "2         0.927500        0.968222   CNN      0.5  0.0005  \n",
      "1         0.926333        0.977167   CNN      0.3  0.0005  \n",
      "10        0.929333        0.968518   CNN      0.3  0.0010  \n",
      "Epoch 1/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 18ms/step - accuracy: 0.7037 - loss: 0.8196 - val_accuracy: 0.8518 - val_loss: 26.6126\n",
      "Epoch 2/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 21ms/step - accuracy: 0.8533 - loss: 0.4351 - val_accuracy: 0.8701 - val_loss: 23.3311\n",
      "Epoch 3/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 23ms/step - accuracy: 0.8846 - loss: 0.3480 - val_accuracy: 0.8649 - val_loss: 18.5874\n",
      "Epoch 4/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.8995 - loss: 0.3028 - val_accuracy: 0.8738 - val_loss: 17.9171\n",
      "Epoch 5/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9110 - loss: 0.2644 - val_accuracy: 0.8779 - val_loss: 17.5904\n",
      "Epoch 6/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 25ms/step - accuracy: 0.9201 - loss: 0.2398 - val_accuracy: 0.8907 - val_loss: 18.4741\n",
      "Epoch 7/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9274 - loss: 0.2163 - val_accuracy: 0.8899 - val_loss: 17.9712\n",
      "Epoch 8/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9341 - loss: 0.1934 - val_accuracy: 0.8707 - val_loss: 21.9416\n",
      "Epoch 9/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9409 - loss: 0.1771 - val_accuracy: 0.8753 - val_loss: 26.5138\n",
      "Epoch 10/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9477 - loss: 0.1583 - val_accuracy: 0.8745 - val_loss: 27.3585\n",
      "Epoch 11/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9514 - loss: 0.1413 - val_accuracy: 0.8927 - val_loss: 28.0347\n",
      "Epoch 12/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9573 - loss: 0.1278 - val_accuracy: 0.8830 - val_loss: 30.5029\n",
      "Epoch 13/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9613 - loss: 0.1147 - val_accuracy: 0.8922 - val_loss: 32.0500\n",
      "Epoch 14/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9635 - loss: 0.1045 - val_accuracy: 0.8945 - val_loss: 40.5339\n",
      "Epoch 15/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 25ms/step - accuracy: 0.9686 - loss: 0.0940 - val_accuracy: 0.8767 - val_loss: 64.1978\n",
      "Epoch 1/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 26ms/step - accuracy: 0.7730 - loss: 0.6280 - val_accuracy: 0.8645 - val_loss: 29.6456\n",
      "Epoch 2/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 25ms/step - accuracy: 0.8759 - loss: 0.3505 - val_accuracy: 0.8818 - val_loss: 20.6092\n",
      "Epoch 3/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.8989 - loss: 0.2886 - val_accuracy: 0.8860 - val_loss: 16.5727\n",
      "Epoch 4/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9108 - loss: 0.2520 - val_accuracy: 0.8730 - val_loss: 16.8904\n",
      "Epoch 5/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9215 - loss: 0.2196 - val_accuracy: 0.8848 - val_loss: 19.6920\n",
      "Epoch 6/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9319 - loss: 0.1929 - val_accuracy: 0.8835 - val_loss: 19.4016\n",
      "Epoch 7/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9396 - loss: 0.1710 - val_accuracy: 0.8999 - val_loss: 19.2210\n",
      "Epoch 8/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9463 - loss: 0.1501 - val_accuracy: 0.8909 - val_loss: 25.6738\n",
      "Epoch 9/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9512 - loss: 0.1346 - val_accuracy: 0.8994 - val_loss: 22.5755\n",
      "Epoch 10/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9589 - loss: 0.1131 - val_accuracy: 0.8921 - val_loss: 27.2375\n",
      "Epoch 11/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9634 - loss: 0.1024 - val_accuracy: 0.8750 - val_loss: 35.4036\n",
      "Epoch 12/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9684 - loss: 0.0892 - val_accuracy: 0.8590 - val_loss: 43.8784\n",
      "Epoch 13/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9716 - loss: 0.0782 - val_accuracy: 0.8573 - val_loss: 58.4788\n",
      "Epoch 14/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9738 - loss: 0.0720 - val_accuracy: 0.8876 - val_loss: 46.9093\n",
      "Epoch 15/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9772 - loss: 0.0645 - val_accuracy: 0.8801 - val_loss: 42.2146\n",
      "Epoch 1/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 24ms/step - accuracy: 0.7988 - loss: 0.5633 - val_accuracy: 0.8474 - val_loss: 25.2774\n",
      "Epoch 2/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.8830 - loss: 0.3320 - val_accuracy: 0.8698 - val_loss: 22.5400\n",
      "Epoch 3/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9013 - loss: 0.2793 - val_accuracy: 0.8816 - val_loss: 15.5695\n",
      "Epoch 4/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9135 - loss: 0.2456 - val_accuracy: 0.8804 - val_loss: 20.4821\n",
      "Epoch 5/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9214 - loss: 0.2201 - val_accuracy: 0.8785 - val_loss: 18.2233\n",
      "Epoch 6/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9320 - loss: 0.1945 - val_accuracy: 0.8942 - val_loss: 16.0663\n",
      "Epoch 7/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9368 - loss: 0.1766 - val_accuracy: 0.8450 - val_loss: 39.2488\n",
      "Epoch 8/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9427 - loss: 0.1599 - val_accuracy: 0.8662 - val_loss: 33.0919\n",
      "Epoch 9/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 25ms/step - accuracy: 0.9480 - loss: 0.1439 - val_accuracy: 0.8784 - val_loss: 35.3648\n",
      "Epoch 10/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 25ms/step - accuracy: 0.9537 - loss: 0.1271 - val_accuracy: 0.8713 - val_loss: 34.8048\n",
      "Epoch 11/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9575 - loss: 0.1172 - val_accuracy: 0.8534 - val_loss: 43.3582\n",
      "Epoch 12/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9618 - loss: 0.1050 - val_accuracy: 0.8813 - val_loss: 51.1653\n",
      "Epoch 13/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9630 - loss: 0.1045 - val_accuracy: 0.8409 - val_loss: 56.5891\n",
      "Epoch 14/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9644 - loss: 0.0978 - val_accuracy: 0.8757 - val_loss: 51.4968\n",
      "Epoch 15/15\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 24ms/step - accuracy: 0.9688 - loss: 0.0859 - val_accuracy: 0.8532 - val_loss: 65.5017\n",
      "[0.8945000171661377, 0.8999000191688538, 0.8942000269889832]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cnn_df['model'] = 'CNN'\n",
    "cnn_df['lr'] = [0.005/10] * 9 + [0.01/10] * 9 + [0.015/10] * 9 + [0.02/10] * 9\n",
    "vanilla_df['model'] = 'vanilla NN'\n",
    "total_df = pd.concat([cnn_df, vanilla_df])\n",
    "\n",
    "best_configs = total_df.nlargest(3, 'best_valid_acc')\n",
    "print(best_configs)\n",
    "\n",
    "##### Calculate the performance on the tesst set to really determine the performance of all the configurations to see which are the best ######\n",
    "\n",
    "test_performances = []\n",
    "\n",
    "for i in range(3):\n",
    "    config = best_configs.iloc[i]\n",
    "    optim = AdamW(learning_rate=config['lr'])\n",
    "    model = CNN(initialization=RandomNormal(), activation=config['activation_function'], optimizer=optim, regularization=None, dropout=config['dropout'])\n",
    "    hist = model.fit_model(X_train, y_train, epochs=15, validation_data=[X_test, y_test])\n",
    "    test_performances.append(max(hist.history['val_accuracy']))\n",
    "\n",
    "print(test_performances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09446eb",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "All top-3 are CNN with a relu activation funciton and a dropout and a relatively low learning rate.\n",
    "\n",
    "The best accuracy of the top-3 configurations is 0.89450, 0.89990, 0.89420, which is around 3% lower than on the validation set. This is quite good we suppose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c562349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 23ms/step - accuracy: 0.2587 - loss: 1.9524 - val_accuracy: 0.3822 - val_loss: 1.6667\n",
      "Epoch 2/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 26ms/step - accuracy: 0.4330 - loss: 1.5433 - val_accuracy: 0.5284 - val_loss: 1.3019\n",
      "Epoch 3/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 30ms/step - accuracy: 0.5397 - loss: 1.3009 - val_accuracy: 0.6274 - val_loss: 1.0625\n",
      "Epoch 4/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.6184 - loss: 1.1062 - val_accuracy: 0.6802 - val_loss: 0.9426\n",
      "Epoch 5/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 32ms/step - accuracy: 0.6762 - loss: 0.9621 - val_accuracy: 0.6884 - val_loss: 0.9060\n",
      "Epoch 6/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 31ms/step - accuracy: 0.7200 - loss: 0.8431 - val_accuracy: 0.7322 - val_loss: 0.8119\n",
      "Epoch 7/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.7494 - loss: 0.7539 - val_accuracy: 0.7328 - val_loss: 0.8078\n",
      "Epoch 8/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.7802 - loss: 0.6662 - val_accuracy: 0.7444 - val_loss: 0.8062\n",
      "Epoch 9/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.8029 - loss: 0.5982 - val_accuracy: 0.7440 - val_loss: 0.8164\n",
      "Epoch 10/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.8280 - loss: 0.5264 - val_accuracy: 0.7622 - val_loss: 0.7767\n",
      "Epoch 11/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.8505 - loss: 0.4631 - val_accuracy: 0.7619 - val_loss: 0.8197\n",
      "Epoch 12/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.8642 - loss: 0.4170 - val_accuracy: 0.7586 - val_loss: 0.8349\n",
      "Epoch 13/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.8805 - loss: 0.3708 - val_accuracy: 0.7569 - val_loss: 0.8647\n",
      "Epoch 14/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.8930 - loss: 0.3329 - val_accuracy: 0.7643 - val_loss: 0.9205\n",
      "Epoch 15/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9029 - loss: 0.2994 - val_accuracy: 0.7544 - val_loss: 0.9716\n",
      "Epoch 16/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9112 - loss: 0.2804 - val_accuracy: 0.7597 - val_loss: 0.9581\n",
      "Epoch 17/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 30ms/step - accuracy: 0.9226 - loss: 0.2486 - val_accuracy: 0.7608 - val_loss: 1.0490\n",
      "Epoch 18/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.9252 - loss: 0.2366 - val_accuracy: 0.7409 - val_loss: 1.2124\n",
      "Epoch 19/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.9333 - loss: 0.2141 - val_accuracy: 0.7693 - val_loss: 1.0482\n",
      "Epoch 20/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9383 - loss: 0.1983 - val_accuracy: 0.7697 - val_loss: 1.1110\n",
      "Epoch 21/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9395 - loss: 0.2000 - val_accuracy: 0.7702 - val_loss: 1.0722\n",
      "Epoch 22/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9473 - loss: 0.1684 - val_accuracy: 0.7634 - val_loss: 1.2432\n",
      "Epoch 23/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9455 - loss: 0.1854 - val_accuracy: 0.7552 - val_loss: 1.2081\n",
      "Epoch 24/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9509 - loss: 0.1625 - val_accuracy: 0.7654 - val_loss: 1.1325\n",
      "Epoch 25/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.9533 - loss: 0.1569 - val_accuracy: 0.7705 - val_loss: 1.2109\n",
      "Epoch 1/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 32ms/step - accuracy: 0.3660 - loss: 1.7202 - val_accuracy: 0.4923 - val_loss: 1.3856\n",
      "Epoch 2/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 31ms/step - accuracy: 0.5609 - loss: 1.2430 - val_accuracy: 0.6424 - val_loss: 1.0260\n",
      "Epoch 3/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.6544 - loss: 0.9991 - val_accuracy: 0.7061 - val_loss: 0.8506\n",
      "Epoch 4/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.7192 - loss: 0.8298 - val_accuracy: 0.7393 - val_loss: 0.7772\n",
      "Epoch 5/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.7637 - loss: 0.7026 - val_accuracy: 0.7388 - val_loss: 0.7775\n",
      "Epoch 6/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.8020 - loss: 0.5906 - val_accuracy: 0.7482 - val_loss: 0.7707\n",
      "Epoch 7/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.8332 - loss: 0.4943 - val_accuracy: 0.7670 - val_loss: 0.7252\n",
      "Epoch 8/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.8617 - loss: 0.4101 - val_accuracy: 0.7700 - val_loss: 0.7534\n",
      "Epoch 9/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 30ms/step - accuracy: 0.8839 - loss: 0.3481 - val_accuracy: 0.7732 - val_loss: 0.7712\n",
      "Epoch 10/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.9041 - loss: 0.2892 - val_accuracy: 0.7761 - val_loss: 0.7775\n",
      "Epoch 11/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.9190 - loss: 0.2438 - val_accuracy: 0.7754 - val_loss: 0.8477\n",
      "Epoch 12/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.9272 - loss: 0.2174 - val_accuracy: 0.7751 - val_loss: 0.9194\n",
      "Epoch 13/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.9395 - loss: 0.1868 - val_accuracy: 0.7630 - val_loss: 0.9582\n",
      "Epoch 14/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.9465 - loss: 0.1656 - val_accuracy: 0.7623 - val_loss: 1.0711\n",
      "Epoch 15/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 30ms/step - accuracy: 0.9507 - loss: 0.1520 - val_accuracy: 0.7585 - val_loss: 1.1671\n",
      "Epoch 16/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9533 - loss: 0.1440 - val_accuracy: 0.7694 - val_loss: 1.0452\n",
      "Epoch 17/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9599 - loss: 0.1270 - val_accuracy: 0.7610 - val_loss: 1.2318\n",
      "Epoch 18/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 30ms/step - accuracy: 0.9612 - loss: 0.1212 - val_accuracy: 0.7656 - val_loss: 1.1323\n",
      "Epoch 19/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.9621 - loss: 0.1160 - val_accuracy: 0.7690 - val_loss: 1.1960\n",
      "Epoch 20/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9660 - loss: 0.1032 - val_accuracy: 0.7679 - val_loss: 1.1587\n",
      "Epoch 21/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 31ms/step - accuracy: 0.9667 - loss: 0.1033 - val_accuracy: 0.7543 - val_loss: 1.2945\n",
      "Epoch 22/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 32ms/step - accuracy: 0.9684 - loss: 0.1000 - val_accuracy: 0.7683 - val_loss: 1.2610\n",
      "Epoch 23/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.9703 - loss: 0.0948 - val_accuracy: 0.7624 - val_loss: 1.3085\n",
      "Epoch 24/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.9709 - loss: 0.0919 - val_accuracy: 0.7682 - val_loss: 1.2482\n",
      "Epoch 25/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9722 - loss: 0.0903 - val_accuracy: 0.7663 - val_loss: 1.3418\n",
      "Epoch 1/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 31ms/step - accuracy: 0.3364 - loss: 1.7667 - val_accuracy: 0.4917 - val_loss: 1.3801\n",
      "Epoch 2/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.5262 - loss: 1.3278 - val_accuracy: 0.6065 - val_loss: 1.1365\n",
      "Epoch 3/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 30ms/step - accuracy: 0.6198 - loss: 1.0961 - val_accuracy: 0.6546 - val_loss: 0.9919\n",
      "Epoch 4/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.6806 - loss: 0.9353 - val_accuracy: 0.6856 - val_loss: 0.9210\n",
      "Epoch 5/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.7267 - loss: 0.8082 - val_accuracy: 0.6981 - val_loss: 0.8875\n",
      "Epoch 6/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.7571 - loss: 0.7149 - val_accuracy: 0.7197 - val_loss: 0.8404\n",
      "Epoch 7/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.7898 - loss: 0.6214 - val_accuracy: 0.7308 - val_loss: 0.8345\n",
      "Epoch 8/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.8138 - loss: 0.5543 - val_accuracy: 0.7343 - val_loss: 0.8464\n",
      "Epoch 9/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.8386 - loss: 0.4759 - val_accuracy: 0.7360 - val_loss: 0.9286\n",
      "Epoch 10/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.8547 - loss: 0.4360 - val_accuracy: 0.7388 - val_loss: 0.8767\n",
      "Epoch 11/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.8740 - loss: 0.3809 - val_accuracy: 0.7398 - val_loss: 0.9538\n",
      "Epoch 12/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.8850 - loss: 0.3433 - val_accuracy: 0.7421 - val_loss: 0.9379\n",
      "Epoch 13/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 30ms/step - accuracy: 0.8981 - loss: 0.3076 - val_accuracy: 0.7377 - val_loss: 1.0196\n",
      "Epoch 14/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.9050 - loss: 0.2922 - val_accuracy: 0.7371 - val_loss: 1.0276\n",
      "Epoch 15/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9117 - loss: 0.2694 - val_accuracy: 0.7372 - val_loss: 1.0578\n",
      "Epoch 16/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9177 - loss: 0.2494 - val_accuracy: 0.7373 - val_loss: 1.1547\n",
      "Epoch 17/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 31ms/step - accuracy: 0.9225 - loss: 0.2371 - val_accuracy: 0.7324 - val_loss: 1.1761\n",
      "Epoch 18/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 32ms/step - accuracy: 0.9260 - loss: 0.2319 - val_accuracy: 0.7272 - val_loss: 1.2549\n",
      "Epoch 19/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - accuracy: 0.9324 - loss: 0.2089 - val_accuracy: 0.7306 - val_loss: 1.2043\n",
      "Epoch 20/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9383 - loss: 0.1965 - val_accuracy: 0.7459 - val_loss: 1.2578\n",
      "Epoch 21/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9383 - loss: 0.1976 - val_accuracy: 0.7371 - val_loss: 1.3511\n",
      "Epoch 22/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9390 - loss: 0.1930 - val_accuracy: 0.7274 - val_loss: 1.2449\n",
      "Epoch 23/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 30ms/step - accuracy: 0.9413 - loss: 0.1897 - val_accuracy: 0.7439 - val_loss: 1.2089\n",
      "Epoch 24/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 30ms/step - accuracy: 0.9474 - loss: 0.1677 - val_accuracy: 0.7422 - val_loss: 1.2902\n",
      "Epoch 25/25\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 30ms/step - accuracy: 0.9483 - loss: 0.1620 - val_accuracy: 0.7332 - val_loss: 1.4100\n",
      "[0.7705000042915344, 0.7760999798774719, 0.7458999752998352]\n"
     ]
    }
   ],
   "source": [
    "cifar_10 = keras.datasets.cifar10 \n",
    "(X_train_full, y_train_full), (X_test, y_test) = cifar_10.load_data() #Load in CIFAR-10 dataset\n",
    "\n",
    "X_train, X_test = X_train_full/255.0, X_test/255.0 #Confert 8-bit int to floating point in [0,1]\n",
    "\n",
    "cnn_df['lr'] = [0.005/10] * 9 + [0.01/10] * 9 + [0.015/10] * 9 + [0.02/10] * 9\n",
    "total_df = pd.concat([cnn_df, vanilla_df])\n",
    "\n",
    "best_configs = total_df.nlargest(3, 'best_valid_acc')\n",
    "test_performances = []\n",
    "for i in range(3):\n",
    "    config = best_configs.iloc[i]\n",
    "    optim = AdamW(learning_rate=config['lr'])\n",
    "    model = CNN(initialization=RandomNormal(), activation=config['activation_function'], optimizer=optim, regularization=None, dropout=config['dropout'], input_shape=(32, 32, 3))\n",
    "    hist = model.fit_model(X_train, y_train_full, epochs=25, validation_data=(X_test, y_test))\n",
    "    test_performances.append(max(hist.history['val_accuracy']))\n",
    "\n",
    "print(test_performances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0379b6aa",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "It is difficult to say it translates directly as it does converge slower. However, it can be possible that using more epochs can improve the results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IDL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
